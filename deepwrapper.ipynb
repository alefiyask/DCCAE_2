{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><u>Implementing Wrapper class for Deep model- deepwrapper </h3><br>\n",
    "This is a wrapper class for Deep CCA\n",
    "We create an instance with a method and number of latent dimensions.\n",
    "\n",
    "The class has a number of methods intended to align roughly with the linear Wrapper:\n",
    "\n",
    "<b>fit():</b></b> gives us train correlations and stores the variables needed for out of sample prediction as well as some\n",
    "method-specific variables\n",
    "\n",
    "<b>transform_view():</b> allows us to transform given views to the latent variable space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><b>Library: <br></b>\n",
    "<b>sklearn.sklearn.cross_decomposition:</b>For implementing Regression Partial least squares regression in 1-D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from torch.utils.data import DataLoader\n",
    "from DCCAE_repo.configuration import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepWrapper class inmplements the <b>fit()</b> and  <b>transform() </b> functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepWrapper:\n",
    "\n",
    "    def __init__(self, config: Config = Config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def fit(self, *args, labels=None, val_split=0.2):\n",
    "        if type(args[0]) is np.ndarray:\n",
    "            dataset = cca_datasets.CCA_Dataset(*args, labels=labels)\n",
    "            lengths = [len(dataset) - int(len(dataset) * val_split), int(len(dataset) * val_split)]\n",
    "            train_dataset, val_dataset = torch.utils.data.random_split(dataset, lengths)\n",
    "        elif type(args[0]) is torch.utils.data.dataset.Subset and len(args) == 2:\n",
    "            train_dataset, val_dataset = args[0], args[1]\n",
    "        elif type(args[0]) is torch.utils.data.Dataset or type(args[0]) is torch.utils.data.dataset.Subset:\n",
    "            dataset = args[0]\n",
    "            lengths = [len(dataset) - int(len(dataset) * val_split), int(len(dataset) * val_split)]\n",
    "            train_dataset, val_dataset = torch.utils.data.random_split(dataset, lengths)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if self.config.batch_size == 0:\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=len(train_dataset), drop_last=True)\n",
    "        else:\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=self.config.batch_size, drop_last=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=len(val_dataset))\n",
    "\n",
    "        self.config.input_sizes = [view.shape[-1] for view in dataset[0][0]]\n",
    "\n",
    "        # First we get the model class.\n",
    "        # These have a forward method which takes data inputs and outputs the variables needed to calculate their\n",
    "        # respective loss. The models also have loss functions as methods but we can also customise the loss by calling\n",
    "        # a_loss_function(model(data))\n",
    "        \n",
    "        self.model = self.config.method(self.config)\n",
    "        num_params = sum(p.numel() for p in self.model.parameters())\n",
    "        print('total parameters: ', num_params)\n",
    "        best_model = copy.deepcopy(self.model.state_dict())\n",
    "        self.model.double().to(self.device)\n",
    "        min_val_loss = torch.tensor(np.inf)\n",
    "        epochs_no_improve = 0\n",
    "        early_stop = False\n",
    "        all_train_loss = []\n",
    "        all_val_loss = []\n",
    "\n",
    "        for epoch in range(1, self.config.epoch_num + 1):\n",
    "            if not early_stop:\n",
    "                epoch_train_loss = self.train_epoch(train_dataloader)\n",
    "                print('====> Epoch: {} Average train loss: {:.4f}'.format(\n",
    "                    epoch, epoch_train_loss))\n",
    "                epoch_val_loss = self.val_epoch(val_dataloader)\n",
    "                print('====> Epoch: {} Average val loss: {:.4f}'.format(\n",
    "                    epoch, epoch_val_loss))\n",
    "\n",
    "                if epoch_val_loss < min_val_loss or epoch == 1:\n",
    "                    min_val_loss = epoch_val_loss\n",
    "                    best_model = copy.deepcopy(self.model.state_dict())\n",
    "                    print('Min loss %0.2f' % min_val_loss)\n",
    "                    epochs_no_improve = 0\n",
    "\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    # Check early stopping condition\n",
    "                    if epochs_no_improve == self.config.patience and self.config.patience > 0:\n",
    "                        print('Early stopping!')\n",
    "                        early_stop = True\n",
    "                        self.model.load_state_dict(best_model)\n",
    "\n",
    "                all_train_loss.append(epoch_train_loss)\n",
    "                all_val_loss.append(epoch_val_loss)\n",
    "       \n",
    "        if not self.config.autoencoder:\n",
    "            self.train_correlations = self.predict_corr(train_dataset, train=True)\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def transform_view(self, *args, labels=None, train=False):\n",
    "        if type(args[0]) is np.ndarray:\n",
    "            test_dataset = cca_datasets.CCA_Dataset(*args, labels=labels)\n",
    "        elif type(args[0]) is torch.utils.data.Dataset or type(args[0]) is torch.utils.data.dataset.Subset:\n",
    "            test_dataset = args[0]\n",
    "        else:\n",
    "            pass\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, label) in enumerate(test_dataloader):\n",
    "                data = [d.to(self.device) for d in list(data)]\n",
    "                z = self.model(*data)\n",
    "                if batch_idx == 0:\n",
    "                    z_list = [z_i.detach().cpu().numpy() for i, z_i in enumerate(z)]\n",
    "                else:\n",
    "                    z_list = [np.append(z_list[i], z_i.detach().cpu().numpy(), axis=0) for\n",
    "                              i, z_i in enumerate(z)]\n",
    "        # For trace-norm objective models we need to apply a linear CCA to outputs\n",
    "        if self.config.post_transform:\n",
    "            if train:\n",
    "                self.cca = CCA(n_components=self.config.latent_dims)\n",
    "                z_list = self.cca.fit_transform(z_list[0], z_list[1])\n",
    "            else:\n",
    "                z_list = self.cca.transform(np.array(z_list[0]), np.array(z_list[1]))\n",
    "        return z_list\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
