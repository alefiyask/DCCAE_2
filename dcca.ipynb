{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><u>Deep Canonical Correlation Analysis- Implementation<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code contains the implementation of DCCA.\n",
    "The libaries used are those of torch:<br><br>\n",
    "<b>matmul</b>: Used for matrix multiplication of tensors <br>\n",
    "<b>optim</b>: Used for algorithm optimization<br>\n",
    "<b>nn</b>: for creating and modelling the trainig data into a neural network with configurations mentioned in the config file.<br>\n",
    "<b>functional</b>: Applies a 1D convolution over an input signal composed of several input planes.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim, matmul\n",
    "from torch.nn import functional as F\n",
    "from DCCAE_repo.configuration import Config\n",
    "from DCCAE_repo.objectives import compute_matrix_power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>All of the deep architectures have forward methods inherited from pytorch as well as the methods:\n",
    "\n",
    "<b>loss()</b>: which calculates the loss given some inputs and model outputs i.e. loss(inputs,model(inputs))\n",
    "\n",
    "This allows us to wrap them all up in the deep wrapper.It is useful\n",
    "for standardising the pipeline for comparison<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><b>We use the following functions:</b>\n",
    "\n",
    "create_encoder(): For creating the encoder with the given input parameters specified in the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(config, i):\n",
    "    encoder = config.encoder_models[i](config.hidden_layer_sizes[i], config.input_sizes[i], config.latent_dims).double()\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Here we have defined a class DCCA, containing the methods for conduction of multi-view analysis of non-linear data.<br><br>\n",
    "<b>__init__:</b> Here all the parameteres for class objects are assigned. Constructor.<br>\n",
    "<b>encode:</b> Data input for encoding<br>\n",
    "<b>forward:</b> Data is forwarded to the encoder<br>\n",
    "<b>update_weights_tn:</b> To add or alter the associated weights of the nn.<br>\n",
    "<b>tn_loss:</b> To compute the net loss.<br>\n",
    "<b>update_weights_als:</b> Update weights for Alternating Least Squares method.<br>\n",
    "<b>als_loss:</b> Computing loss for Alternating Least Squares method.<br>\n",
    "<b>als_loss_validation:</b> To calculate the loss for Alternating Least Squares methodAlternating Least Squares method.<br>\n",
    "<b>update_covariances:</b> Update the net Covariances.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DCCA(nn.Module):\n",
    "\n",
    "    def __init__(self, config: Config = Config):\n",
    "        super(DCCA, self).__init__()\n",
    "        views = len(config.encoder_models)\n",
    "        self.config = config\n",
    "        self.encoders = nn.ModuleList([create_encoder(config, i) for i in range(views)])\n",
    "        self.objective = config.objective(config.latent_dims)\n",
    "        self.optimizers = [optim.Adam(list(encoder.parameters()), lr=config.learning_rate) for encoder in self.encoders]\n",
    "        self.covs = None\n",
    "        if config.als:\n",
    "            self.update_weights = self.update_weights_als\n",
    "            self.loss = self.als_loss_validation\n",
    "        else:\n",
    "            self.update_weights = self.update_weights_tn\n",
    "            self.loss = self.tn_loss\n",
    "\n",
    "    def encode(self, *args):\n",
    "        z = []\n",
    "        for i, arg in enumerate(args):\n",
    "            z.append(self.encoders[i](arg))\n",
    "        return tuple(z)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        z = self.encode(*args)\n",
    "        return z\n",
    "\n",
    "    def update_weights_tn(self, *args):\n",
    "        [optimizer.zero_grad() for optimizer in self.optimizers]\n",
    "        loss = self.tn_loss(*args)\n",
    "        loss.backward()\n",
    "        [optimizer.step() for optimizer in self.optimizers]\n",
    "        return loss\n",
    "\n",
    "    def tn_loss(self, *args):\n",
    "        z = self(*args)\n",
    "        return self.objective.loss(*z)\n",
    "\n",
    "    def update_weights_als(self, *args):\n",
    "        loss_1, loss_2 = self.als_loss(*args)\n",
    "        self.optimizers[0].zero_grad()\n",
    "        loss_1.backward()\n",
    "        self.optimizers[0].step()\n",
    "        self.optimizers[1].zero_grad()\n",
    "        loss_2.backward()\n",
    "        self.optimizers[1].step()\n",
    "        return (loss_1 + loss_2) / 2 - self.config.latent_dims\n",
    "\n",
    "    def als_loss(self, *args):\n",
    "        z = self(*args)\n",
    "        self.update_covariances(*z)\n",
    "        covariance_inv = [compute_matrix_power(cov, -0.5, self.config.eps) for cov in self.covs]\n",
    "        preds = [matmul(z, covariance_inv[i]).detach() for i, z in enumerate(z)]\n",
    "        \n",
    "        # Least squares for each projection in same manner as linear from before\n",
    "        # Currently 2 view case\n",
    "        losses = [F.mse_loss(preds[-i], z) for i, z in enumerate(z)]\n",
    "        return losses\n",
    "\n",
    "    def als_loss_validation(self, *args):\n",
    "        z = self(*args)\n",
    "        SigmaHat11RootInv = compute_matrix_power(self.covs[0], -0.5, self.config.eps)\n",
    "        SigmaHat22RootInv = compute_matrix_power(self.covs[1], -0.5, self.config.eps)\n",
    "        pred_1 = (z[0] @ SigmaHat11RootInv).detach()\n",
    "        pred_2 = (z[1] @ SigmaHat22RootInv).detach()\n",
    "        # Least squares for each projection in same manner as linear from before\n",
    "        \n",
    "        loss_1 = F.mse_loss(pred_1, z[1])\n",
    "        loss_2 = F.mse_loss(pred_2, z[0])\n",
    "        return (loss_1 + loss_2) / 2 - self.config.latent_dims\n",
    "\n",
    "    def update_covariances(self, *args):\n",
    "        b = args[0].shape[0]\n",
    "        batch_covs = [z_i.T @ z_i / b for i, z_i in enumerate(args)]\n",
    "        if self.covs is not None:\n",
    "            self.covs = [(self.config.rho * self.covs[i]).detach() + (1 - self.config.rho) * batch_cov for i, batch_cov\n",
    "                         in\n",
    "                         enumerate(batch_covs)]\n",
    "        else:\n",
    "            self.covs = batch_covs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
